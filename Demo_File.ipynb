{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo File.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ca8acc3062af4f76a11cf049c52e8448": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8db6d5aee0064cf98e0884478ab67682",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1535fbf654724f30a79a3e3556956d4f",
              "IPY_MODEL_a09d85eaa01f4bdaa333fe6fd4110ff5"
            ]
          }
        },
        "8db6d5aee0064cf98e0884478ab67682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1535fbf654724f30a79a3e3556956d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e94d7cb1d6cd4389bf306e86eadf5eca",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd7517316adb457eb77c680d1ea84b7e"
          }
        },
        "a09d85eaa01f4bdaa333fe6fd4110ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d40944efe8304776adc581a2962ff87f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:01&lt;00:00, 122kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_323178e40712454bb07a5ea1ca2e3191"
          }
        },
        "e94d7cb1d6cd4389bf306e86eadf5eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd7517316adb457eb77c680d1ea84b7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d40944efe8304776adc581a2962ff87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "323178e40712454bb07a5ea1ca2e3191": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "760f786c3f3f44bc9252a2d74cc5833a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3b11f142b47e47c5a6a23d3fa92b86b8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4795d697736645a582a3626644ceb1a9",
              "IPY_MODEL_7a2a9e08e38e457a90b14d961b089ca4"
            ]
          }
        },
        "3b11f142b47e47c5a6a23d3fa92b86b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4795d697736645a582a3626644ceb1a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ee92dd13afd94fee82c9eefa45ecaacb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5f94659984544669414732405ca6200"
          }
        },
        "7a2a9e08e38e457a90b14d961b089ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d65dd9d78d1645ee83665ce8efeaac04",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 44.6B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4cfdca1bbf1b40e19222c68205b22db5"
          }
        },
        "ee92dd13afd94fee82c9eefa45ecaacb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5f94659984544669414732405ca6200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d65dd9d78d1645ee83665ce8efeaac04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4cfdca1bbf1b40e19222c68205b22db5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "473e4355e3ce431782ab098bc87c6e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6857fbd325b2434c9418baf57602666f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7d4407225204478981a5c1fc693903e2",
              "IPY_MODEL_d76a94efd44540deaf1514e25c47d9db"
            ]
          }
        },
        "6857fbd325b2434c9418baf57602666f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d4407225204478981a5c1fc693903e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d5b6ac5416e44973b2dcc144807b9aaa",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a000a74f93e4263bfbea21eb0c7b23a"
          }
        },
        "d76a94efd44540deaf1514e25c47d9db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_33e2d5de996a472890d32d7a65b9b82f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 2.28MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99b59fd4f27b4d9388049ca14880455a"
          }
        },
        "d5b6ac5416e44973b2dcc144807b9aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a000a74f93e4263bfbea21eb0c7b23a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33e2d5de996a472890d32d7a65b9b82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99b59fd4f27b4d9388049ca14880455a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcYo0x884qCF"
      },
      "source": [
        "<h2>Set up instruction<h2>\n",
        "Create a folder in Google Drive COMP0087 your own google drive and inside that folder create 3 more subfolder named DataRequired,  BertModels, and lstmFinalModel.\n",
        "\n",
        "\n",
        "Download the files from this link or create a shortcut to these folders.\n",
        "This is link is for the data set  and paste the content into the folder DataRequired\n",
        "\n",
        "\n",
        "https://drive.google.com/drive/folders/1UmIaTE_dTOSCBf-ardkr9eo6WWQ5oEiz?usp=sharing\n",
        "\n",
        "\n",
        "This is link is for the bert model copy and paste the content into the folder BertModels\n",
        "https://drive.google.com/drive/folders/1vcKPAlvTPQarftyocat1oF-hPQ9eZumE?usp=sharing\n",
        "\n",
        "\n",
        "This is link is for the lstm models copy and paste the content into the folder lstmFinalModel\n",
        "\n",
        "\n",
        " https://drive.google.com/drive/folders/1xlKIV8s1T3FJDTxfVgpCAscr-CSJ7EXk?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEt4teyFNHES",
        "outputId": "79dbec21-b417-4b25-d81f-d684353b24e3"
      },
      "source": [
        "#!pip install pytorch-pretrained-bert pytorch-nlp\n",
        "#from pytorch_pretrained_bert import BertConfig\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup, BertConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import itertools\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('drive/MyDrive/COMP0087/')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 7.8MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 46.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9uw8-6y4aUT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EePXbu23RQ24"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HYZ_euaNbX9"
      },
      "source": [
        "LSTM Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EQodDoeOcDj",
        "outputId": "11b4a9de-97b3-4656-d24d-b3bfdcee2ad7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk \n",
        "from nltk.corpus import stopwords                   \n",
        "from nltk.stem import PorterStemmer \n",
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer          \n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fd62794a3d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLS2kqGSNdec",
        "outputId": "f46116bb-c74d-47e3-d2ea-79d2140fb3b0"
      },
      "source": [
        "# In case we wanna use it later\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avrJAc4HOUxO",
        "outputId": "5fe71292-cc28-44a9-c954-3ab97513f561"
      },
      "source": [
        "# Download w2v\n",
        "%%shell\n",
        "import wget\n",
        "\n",
        "wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: import: command not found\n",
            "--2021-05-31 10:37:18--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.111.102\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.111.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  6.04MB/s    in 7m 6s   \n",
            "\n",
            "2021-05-31 10:44:25 (3.69 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "mDB5rNGAOhQd",
        "outputId": "3a8888a7-a625-4f7f-d3f7-05dd4b4d1d77"
      },
      "source": [
        "%%shell\n",
        "gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gzip: GoogleNews-vectors-negative300.bin already exists; do you wish to overwrite (y or n)? n\n",
            "\tnot overwritten\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e3942604da16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gunzip GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 139\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'gunzip GoogleNews-vectors-negative300.bin.gz' returned non-zero exit status 2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwJpQohkOjyM"
      },
      "source": [
        "model_g = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdGbwEf-OmLQ"
      },
      "source": [
        "# import file \n",
        "df = pd.read_csv(\"DataRequired/training_set_rel3.tsv\", sep='\\t', encoding='ISO-8859-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DST7FIkNOoUX"
      },
      "source": [
        "# create a list of tokens using every word in essay\n",
        "# For testing now, to check if the baseline is working\n",
        "def essay_to_wordlist(essay_v, remove_stopwords):\n",
        "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
        "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
        "    #words = essay_v.lower().split()\n",
        "    words = essay_v.split()\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwI0FKmKOsLb",
        "outputId": "4530af4d-40b9-485e-bdda-19bb7d8f273e"
      },
      "source": [
        "wv = model_g.wv\n",
        "# Just to test, averaged all the word embeddings in the whole paragraph into a single embedding vector.\n",
        "# function to change list of tokens to word embedding using word2vec\n",
        "# not being used\n",
        "def avgW2vRepresentation( passage ):\n",
        "    counterWordsVocab = 0\n",
        "    w2vRepresentation = np.zeros(300)\n",
        "    for words in passage:\n",
        "        if words in wv:\n",
        "            counterWordsVocab +=1\n",
        "            w2vRepresentation += wv[words]\n",
        "\n",
        "    if(counterWordsVocab != 0):\n",
        "        w2vRepresentation /= counterWordsVocab\n",
        "\n",
        "    return w2vRepresentation\n",
        "\n",
        "# is being used\n",
        "def embed_paragraph(passage):\n",
        "  embedded_vect = []\n",
        "  for words in passage:\n",
        "    if words in wv:\n",
        "      embedded_vect.append(wv[words])\n",
        "    \n",
        "  return np.array(embedded_vect)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_r_qmEFOuiQ"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size = 300, hidden_layer_size_1 = 300, hidden_layer_size_2 = 64, output_size = 16, loss_function = None, lr = 0.001, dropout_amount = 0.4):\n",
        "        super().__init__()\n",
        "        self.hidden_layer_size_1 = hidden_layer_size_1\n",
        "        self.hidden_layer_size_2 = hidden_layer_size_2\n",
        "        self.lstm = nn.LSTM(input_size, hidden_layer_size_1, batch_first = True, dropout = dropout_amount) \n",
        "        self.lstm_2 = nn.LSTM(hidden_layer_size_1, hidden_layer_size_2, batch_first = True, dropout = dropout_amount)        \n",
        "        self.linear = nn.Linear(hidden_layer_size_2, output_size)\n",
        "\n",
        "        self.loss = loss_function if loss_function is not None else nn.CrossEntropyLoss()\n",
        "        self.lr = lr\n",
        "\n",
        "        return None\n",
        "        \n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        # print(f\"Input shape {input_seq.shape}\")\n",
        "        # lstm_out, self.hidden_cell = self.lstm(input_seq.view(1, input_seq.shape[0], input_seq.shape[1]))\n",
        "        lstm_out, self.hidden_cell = self.lstm(input_seq)\n",
        "        lstm_out, self.hidden_cell = self.lstm_2(lstm_out)\n",
        "        result_unpacked, lengths_unpacked = pad_packed_sequence(lstm_out, batch_first = True)\n",
        "        linear_input = result_unpacked[np.arange(result_unpacked.shape[0]), lengths_unpacked - 1, :]\n",
        "        # predictions = self.linear(lstm_out.view(input_seq.shape[0], -1))\n",
        "        predictions = self.linear(linear_input)\n",
        "        #predictions = F.relu(predictions)\n",
        "        return predictions\n",
        "\n",
        "def pack_data(data, lengths):\n",
        "  input_tens = torch.tensor(data, dtype = torch.float)\n",
        "  packed_seq = pack_padded_sequence(input_tens, lengths, batch_first = True, enforce_sorted = False)\n",
        "  return packed_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pILx-P0PzAb",
        "outputId": "670e95aa-436a-4de2-8039-89044c49a263"
      },
      "source": [
        "models = []\n",
        "for index  in range(5):\n",
        "  modelLabel = \"lstmFinalModel\"+str(index)\n",
        "  models.append(torch.load(\"lstmModels/\"+modelLabel))\n",
        "  print(modelLabel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lstmFinalModel0\n",
            "lstmFinalModel1\n",
            "lstmFinalModel2\n",
            "lstmFinalModel3\n",
            "lstmFinalModel4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfBfbM_HP6bM"
      },
      "source": [
        "#get all the essay set for that particular essay set\n",
        "def import_data_for_testing(essay_set):\n",
        "  df_train = pd.read_excel('DataRequired/training_set_rel3.xlsx')\n",
        "  \n",
        "  \n",
        "  if essay_set is not None:\n",
        "    df_train = df_train[df_train['essay_set']==essay_set]\n",
        "\n",
        "  \n",
        "  return df_train\n",
        "def getRealScore (df):\n",
        "  dfFormat = df[[df.columns[2], df.columns[6],df.columns[10], df.columns[11], df.columns[12],df.columns[13],\\\n",
        "                df.columns[16],df.columns[17],df.columns[18],df.columns[19] ]]\n",
        "  # prepare test data\n",
        "  test_data_array = np.array(dfFormat)\n",
        "  #print(test_data_array)\n",
        "  N_t,d_t = test_data_array.shape\n",
        "  y_test = np.zeros((N_t, 5))\n",
        "  #x_train[:,0] = train_data_array[:,0]\n",
        "  y_test[:,0] = test_data_array[:,1]\n",
        "  y_test[:,1] = test_data_array[:,2] + test_data_array[:,6]\n",
        "  y_test[:,2] = test_data_array[:,3] + test_data_array[:,7]\n",
        "  y_test[:,3] = test_data_array[:,4] + test_data_array[:,8]\n",
        "  y_test[:,4] = test_data_array[:,5] + test_data_array[:,9]\n",
        "  return y_test\n",
        "def processing (test_data):\n",
        "  tokenized_no_stopwords = []; finalized_essay = []\n",
        "  essays = np.array(test_data['essay'])\n",
        "  essay_1 = essays[0]\n",
        "  for essay in essays:\n",
        "    tokenized_no_stopwords.append(essay_to_wordlist(essay, True))\n",
        "\n",
        "  max_len = 0\n",
        "  for tokenized_essay in tokenized_no_stopwords:\n",
        "    em_par = embed_paragraph(tokenized_essay)\n",
        "    par_len = em_par.shape[0]\n",
        "    if max_len < par_len:\n",
        "      max_len = par_len\n",
        "    finalized_essay.append(em_par)\n",
        "\n",
        "  # Pad anyhow, and ignore with padded\n",
        "  padded_embed = []; lengths = []\n",
        "  for embedding in finalized_essay:\n",
        "    essay_len = embedding.shape[0]\n",
        "    template = np.zeros((max_len, 300))\n",
        "    template[:essay_len, :] = embedding\n",
        "    padded_embed.append(template)\n",
        "    lengths.append(essay_len)\n",
        "  test_data = pack_data(padded_embed, lengths)\n",
        "  return test_data\n",
        "\n",
        "def evaluate_model_demo(models, data):\n",
        "  input = data.to(device); scores = []\n",
        "  for model_idx in range(len(models)):\n",
        "    model = models[model_idx]\n",
        "    preds = model(input).cpu().detach().numpy()\n",
        "    preds = np.argmax(preds, axis = 1).reshape(-1)\n",
        "    #label = labels[:, model_idx].reshape(-1)\n",
        "    # if score is 'kappa':\n",
        "    #   pred_score = cohen_kappa_score(preds, label, weights = 'quadratic')\n",
        "    # elif score is 'acc':\n",
        "    #   pred_score = np.mean(preds == label)\n",
        "    \n",
        "    scores.append(preds)\n",
        "\n",
        "  return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ4O-RXERkRp"
      },
      "source": [
        "#getting data for essay set 7\n",
        "dfOnEssaySet = import_data_for_testing(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-oaGWuHP7_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09cafd5c-67ab-423c-b80e-57fa6f59dbe2"
      },
      "source": [
        "#set true to get a random essay if else set random to false and input the desired essay id in the chosen id\n",
        "random = False\n",
        "chosenId = 18121\n",
        "if(random):\n",
        "  chosenId = np.random.choice(dfOnEssaySet['essay_id'])\n",
        "\n",
        "essaydf =  dfOnEssaySet[dfOnEssaySet['essay_id'] == chosenId ]\n",
        "print(essaydf)\n",
        "essay = essaydf['essay'].values\n",
        "\n",
        "print(essay[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       essay_id  essay_set  ... rater3_trait5  rater3_trait6\n",
            "10941     18121          7  ...           NaN            NaN\n",
            "\n",
            "[1 rows x 28 columns]\n",
            "I am never paitent, but which I am it is only once and a while. Sitting down and not doing a thing drives me to be insane. When I’m doing nothing and it is really quiet. I get really bad headaches. In school for example and I’m waiting I get rigily like I tap my foot at tap my pencil and my desk. Tap ???everything things like that. I must do something or I go insane. Waiting in a doctors office, or flying on a plane for a while can get real boring. When I wait to long I feel lazy and tired because I’m not moving at all. I get buggy when I’m paitent, and waiting for something. It is usually no longer than @NUM1 min (half an hour) before I get real bad. I can’t be paitent very long or I get buggy. One way I help myself is playing sports and doing things constantive.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyN-L7Gm3NAX"
      },
      "source": [
        "Predicted result and actual result from lstm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZDTDdX4QfOp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "417bb509-9267-4c13-a3fa-3a2ede5cb3ff"
      },
      "source": [
        "predictedScore = evaluate_model_demo(models,processing(essaydf))\n",
        "print(\"Predicted Score\")\n",
        "print(predictedScore)\n",
        "actualScore = getRealScore(essaydf)\n",
        "print(\"Actual Score\")\n",
        "print(actualScore)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Score\n",
            "[array([20]), array([2]), array([4]), array([4]), array([5])]\n",
            "Actual Score\n",
            "[[14.  2.  2.  4.  6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdOLsVpQ3VnQ"
      },
      "source": [
        "To get predicted result for any essay using lstm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwa713oOQgBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36c5d8c-781a-4d44-c6e3-76236266d46b"
      },
      "source": [
        "#inputting your own essay in the dictionary essay below\n",
        "dictionaryEssay = {}\n",
        "dictionaryEssay['essay'] = [\"I am never paitent, but which I am it is only once and a while. Sitting down and not doing a thing drives me to be insane. When I’m doing nothing and it is really quiet. I get really bad headaches. In school for example and I’m waiting I get rigily like I tap my foot at tap my pencil and my desk. Tap ???everything things like that. I must do something or I go insane. Waiting in a doctors office, or flying on a plane for a while can get real boring. When I wait to long I feel lazy and tired because I’m not moving at all. I get buggy when I’m paitent, and waiting for something. It is usually no longer than @NUM1 min (half an hour) before I get real bad. I can’t be paitent very long or I get buggy. One way I help myself is playing sports and doing things constantive.\"]\n",
        "print(dictionaryEssay)\n",
        "dfown = pd.DataFrame.from_dict(dictionaryEssay)\n",
        "predictedScore = evaluate_model_demo(models,processing(dfown))\n",
        "print(\"Predicted Score\")\n",
        "print(predictedScore)\n",
        "#For multiple model the format is the following\n",
        "# overall score, trait1, trait 2, trait 3, trait 4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'essay': ['I am never paitent, but which I am it is only once and a while. Sitting down and not doing a thing drives me to be insane. When I’m doing nothing and it is really quiet. I get really bad headaches. In school for example and I’m waiting I get rigily like I tap my foot at tap my pencil and my desk. Tap ???everything things like that. I must do something or I go insane. Waiting in a doctors office, or flying on a plane for a while can get real boring. When I wait to long I feel lazy and tired because I’m not moving at all. I get buggy when I’m paitent, and waiting for something. It is usually no longer than @NUM1 min (half an hour) before I get real bad. I can’t be paitent very long or I get buggy. One way I help myself is playing sports and doing things constantive.']}\n",
            "Predicted Score\n",
            "[array([20]), array([2]), array([4]), array([4]), array([5])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO-y5aCBQkHI"
      },
      "source": [
        "Bert model Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UshKLoo3QoAg"
      },
      "source": [
        "def tokenize_sentences(sentences, max_len):\n",
        "  sentences = [\"[CLS] \" + query + \" [SEP]\" for query in sentences]\n",
        "  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "  text_len = torch.LongTensor([len(txt) for txt in tokenized_texts])\n",
        "  input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  input_ids[input_ids[:,-1]!=0,-1] = tokenizer.sep_token_id  #if a sentence has been cut short, make sure last token is SEP\n",
        "  input_ids = torch.LongTensor(input_ids)\n",
        "\n",
        "  #attention masks define which tokens the model pays attention to\n",
        "  attention_masks = np.ones(input_ids.shape)\n",
        "  attention_masks = torch.LongTensor(attention_masks)\n",
        "  attention_masks[input_ids==0] = 0\n",
        "\n",
        "  return input_ids, attention_masks, text_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaVKJs9bQwl6"
      },
      "source": [
        "class EssayModel(nn.Module):\n",
        "  \"\"\"Model for essay grading using BERT embeddings\"\"\"\n",
        "  def __init__(self, hidden_dim, encode_question, regression, dropout, \n",
        "               tokenized_prompt, attention_prompt, output_shape=1):\n",
        "    super().__init__()\n",
        "    self.bert_dim = 768\n",
        "    self.encode_question = encode_question    \n",
        "    self.regression = regression\n",
        "    self.output_shape = output_shape\n",
        "    self.dropout = dropout\n",
        "    self.tokenized_prompt = tokenized_prompt\n",
        "    self.attention_prompt = attention_prompt\n",
        "\n",
        "    if self.encode_question:\n",
        "      self.hidden_dim = hidden_dim\n",
        "    else:\n",
        "      self.hidden_dim = self.bert_dim\n",
        "\n",
        "    #network layers\n",
        "    configuration = BertConfig()\n",
        "    configuration.attention_probs_dropout_prob = dropout\n",
        "    configuration.hidden_dropout_prob = dropout\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, config=configuration).to(device)    \n",
        "    #print(self.bert)\n",
        "    \n",
        "    if self.encode_question:\n",
        "      self.l1 = nn.Linear(in_features=2*self.bert_dim, out_features=self.hidden_dim)\n",
        "      self.BatchNorm = nn.BatchNorm1d(self.hidden_dim)\n",
        "\n",
        "    self.l2 = nn.Linear(in_features=self.hidden_dim, out_features=self.output_shape)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, input_ids, attention_masks):\n",
        "    x = self.bert(input_ids, attention_masks)['pooler_output']\n",
        "    if self.encode_question:\n",
        "      #tokenized_prompt = tokenized_prompt.expand(x.shape[0], -1)\n",
        "      prompt_encoding = self.bert(self.tokenized_prompt, self.attention_prompt)['pooler_output']\n",
        "      prompt_encoding = prompt_encoding.expand(x.shape[0], -1)\n",
        "      x = torch.cat([x, prompt_encoding], dim=1)\n",
        "      x = F.relu(self.BatchNorm(self.l1(x)))\n",
        "    \n",
        "    #if a classification task, apply softmax activation\n",
        "    if self.regression=='reg':\n",
        "      x = self.l2(x)\n",
        "    elif self.regression=='multi':\n",
        "      x = self.l2(x)\n",
        "      x = torch.cat([x,x.sum(1).view(-1,1)], axis=1)  #append total grade to the end\n",
        "    elif self.regression=='class':\n",
        "      x = self.l2(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05-CUHS3VzXh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePUAzXGeWkjA"
      },
      "source": [
        "Set Qns number the bert model is from\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG6j1BuCWpZ3"
      },
      "source": [
        "qnsNumber = 2\n",
        "encode_question = False\n",
        "if(qnsNumber == 1):\n",
        "  lr = 1e-5\n",
        "  \n",
        "  reg = 'reg'\n",
        "  dropout= 0.1\n",
        "  model_label = 'q'+str(qnsNumber)+'_encode{0}_alpha{1}_reg{2}_dropout{3}'.format(encode_question, lr, reg, dropout)\n",
        "elif(qnsNumber == 2):\n",
        "  lr = 1e-5\n",
        "  reg = 'reg'\n",
        "  dropout= 0.1\n",
        "  model_label = 'q'+str(qnsNumber)+'_encode{0}_alpha{1}_reg{2}_dropout{3}'.format(encode_question, lr, reg, dropout)\n",
        "elif(qnsNumber == 3):\n",
        "  lr = 1e-5\n",
        "  reg = 'reg'\n",
        "  dropout= 0.1\n",
        "  model_label = 'q'+str(qnsNumber)+'_encode{0}_alpha{1}_reg{2}_dropout{3}'.format(encode_question, lr, reg, dropout)\n",
        "elif(qnsNumber == 4):\n",
        "  lr = 1e-5\n",
        "  reg = 'reg'\n",
        "  dropout= 0.1\n",
        "  model_label = 'q'+str(qnsNumber)+'_encode{0}_alpha{1}_reg{2}_dropout{3}'.format(encode_question, lr, reg, dropout)\n",
        "elif(qnsNumber == 5):\n",
        "  lr = 1e-5\n",
        "  reg = 'reg'\n",
        "  dropout= 0.1\n",
        "  model_label = 'q'+str(qnsNumber)+'_encode{0}_alpha{1}_reg{2}_dropout{3}'.format(encode_question, lr, reg, dropout)\n",
        "elif(qnsNumber == 6):\n",
        "  lr = 1e-5\n",
        "  reg = 'reg'\n",
        "  dropout= 0.1\n",
        "  model_label = 'q'+str(qnsNumber)+'_encode{0}_alpha{1}_reg{2}_dropout{3}'.format(encode_question, lr, reg, dropout)\n",
        "elif(qnsNumber == 7):\n",
        "  lr = 2e-5\n",
        "  reg = 'reg'\n",
        "  dropout= 0.1\n",
        "  model_label = 'q'+str(qnsNumber)+'_encode{0}_alpha{1}_reg{2}_dropout{3}'.format(encode_question, lr, reg, dropout)\n",
        "  lr = 2e-5\n",
        "  reg = 'multi'\n",
        "  dropout= 0.1\n",
        "  model_label2 = 'q'+str(qnsNumber)+'_encode{0}_alpha{1}_reg{2}_dropout{3}'.format(encode_question, lr, reg, dropout)\n",
        "elif(qnsNumber == 8):\n",
        "  lr = 2e-5\n",
        "  reg = 'reg'\n",
        "  dropout= 0.1\n",
        "  model_label = 'q'+str(qnsNumber)+'_encode{0}_alpha{1}_reg{2}_dropout{3}'.format(encode_question, lr, reg, dropout)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sae_z4E-Q6JL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb65eda3-2888-4e7d-b333-a462bedc3f96"
      },
      "source": [
        "print(model_label)\n",
        "# encode_question = False\n",
        "# lr = 2e-5\n",
        "# reg = 'reg'\n",
        "# dropout= 0.1\n",
        "# model = EssayModel(hidden_dim=512, encode_question=encode_question, \n",
        "#                        regression=reg, output_shape=output_shape, \n",
        "#                        dropout=dropout, tokenized_prompt=tokenized_prompt,\n",
        "#                        attention_prompt=attention_prompt).to(device)\n",
        "# model_label = 'q'+str(qnsNumber)+'_encode{0}_alpha{1}_reg{2}_dropout{3}'.format(encode_question, lr, reg, dropout)\n",
        "model = torch.load(\"BertModels/\"+model_label )\n",
        "model.eval()\n",
        "if (qnsNumber ==7):\n",
        "  print(model_label2)\n",
        "  model2 = torch.load(model_label2 )\n",
        "  model2.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "q2_encodeFalse_alpha1e-05_regreg_dropout0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txo8FN3v2cj1"
      },
      "source": [
        "Randomly or Select a particular essay id from the essay set qns specified above\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRNvL_UwSb-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d6ba41f-4db6-4055-ec8b-6816718c03b0"
      },
      "source": [
        "dfOnEssaySet = import_data_for_testing(qnsNumber)\n",
        "#dfOnEssaySet.head()\n",
        "# dfOnEssaySet.head()\n",
        "#set true to get a random essay if else set random to false and input the desired essay id in the chosen id\n",
        "random = True\n",
        "chosenId = 18121\n",
        "if(random):\n",
        "  chosenId = np.random.choice(dfOnEssaySet['essay_id'])\n",
        "\n",
        "essaydf =  dfOnEssaySet[dfOnEssaySet['essay_id'] == chosenId ]\n",
        "print(essaydf)\n",
        "essay = essaydf['essay'].values\n",
        "\n",
        "print(essay[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      essay_id  essay_set  ... rater3_trait5  rater3_trait6\n",
            "3013      4208          2  ...           NaN            NaN\n",
            "\n",
            "[1 rows x 28 columns]\n",
            "One @MONTH1 subject the censorship of library content in certain places and times, whether fictional or not, to his or her own opinion.  I personally disagree with the concept.  The facts point out that society fears the dystopia predicted by publications exploring the outcome of similar campaigns and has instated systems designed to prevent such occurrences.  Certain examples stand in play to explicate my opinion.       Society, at least in my native @LOCATION1 and surely abroad in other democratic countries, dislikes enforced censorship for a few reasons.  First, many countries have a @CAPS1 of @CAPS2 and priviledges upheld in the face of the law.  Our founding fathers wrote out one of the documents establishing this trend on the prompting of the general public.  The first ten ammendments to our fine constitution, bundled as the @CAPS1 of @CAPS2, resulted.       Some of the included statements ensure the freedom to speak as it publicly pleases us as citizens and the freedom to write, individually and collectively without boundaries.  The removal of these principles so ingrained in our nation's conscience would cause a general uproar.  We as a people instituted our government to preserve our freedoms, and we could prepare ourselves to replace it as implied in the @CAPS3 of @CAPS4 if so required of us to continue doing just that.       Secondly, publications such as the ones mentioned for banning rebuke the enforcement of opinion for its effects on society.  I can mention three books portraying this dystopic outlook on censorship.  '@CAPS5 @NUM1,' required by state standards for one possible choice of reading assigned to us by my teacher, describes book burning and utmost uniformity between citizens.  '@DATE1,' by @PERSON2, I know describes a dystopia in which all citizens completely submit their will to and omnipotent '@CAPS6 @CAPS7,' who utilizes omnicient surveillanceeven though I have not read the composition myself.  '@CAPS8 @CAPS7,' a recent novel by @PERSON1 whose title employs a pun on the name of @DATE1's prime antagonist, weaves the exciting tale of an older teenager who tries to beat the system of surveillance from under the radar.  All, however, betray a sense of fear in their content, and that fear has transferred directly into local civilization.       In reviewing the aspects of censorship portrayed in its effects, the system itself reveals that the even the intended creation of a utopia and compliance through censorship @MONTH1 result in decline and misery throughout the area within which government or civil officials practice the method.  In any case, a more effective solution must exist today.  To prove the point, let me introduce some content advisory systems currently in place.       Libraries divide themselves into sections for the various age groups, and therefore create an inherent barrier between differing groups of content deemed appropriate only in the face of maturity.  The film and television industries rely on ratings displayed at various times during video to provide the same sort of psychological fence dividing content into pieces.  Even the video game industry, into which I plan to inject myself upon exiting college, has a system in place to separate existing media into tiered subcategories designed in a similar way to film ratings.  Different regions utilize different systems, but that of  the @ORGANIZATION3, or the @ORGANIZATION1 for the @ORGANIZATION2, compares relatively well in ranking video games to the rest of the globe.       As explained, censorship already exists in a form accptable to all.  Nobody forces anybody to comply to the recommendations, but hopefully everyone does so on their honor.  Capable parents most definely follow the guidelines which have cropped up over the years.  Only if these methods of moderation were enforced would everyone be upset.  It's up to everyone to prevent a situation like unto the worst-case scenario and useresponsibly the systems available for this purpose\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL2XhkzaQ6_c"
      },
      "source": [
        "\n",
        "\n",
        "#demo_data = torch.utils.data.TensorDataset(input_ids, attention_masks, np.array(essaydf['rater1_domain1'].values[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz-aHbpL2olP"
      },
      "source": [
        "Show the result of prediction of Bert and the actual result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9n72AIRRHC_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236,
          "referenced_widgets": [
            "ca8acc3062af4f76a11cf049c52e8448",
            "8db6d5aee0064cf98e0884478ab67682",
            "1535fbf654724f30a79a3e3556956d4f",
            "a09d85eaa01f4bdaa333fe6fd4110ff5",
            "e94d7cb1d6cd4389bf306e86eadf5eca",
            "bd7517316adb457eb77c680d1ea84b7e",
            "d40944efe8304776adc581a2962ff87f",
            "323178e40712454bb07a5ea1ca2e3191",
            "760f786c3f3f44bc9252a2d74cc5833a",
            "3b11f142b47e47c5a6a23d3fa92b86b8",
            "4795d697736645a582a3626644ceb1a9",
            "7a2a9e08e38e457a90b14d961b089ca4",
            "ee92dd13afd94fee82c9eefa45ecaacb",
            "a5f94659984544669414732405ca6200",
            "d65dd9d78d1645ee83665ce8efeaac04",
            "4cfdca1bbf1b40e19222c68205b22db5",
            "473e4355e3ce431782ab098bc87c6e52",
            "6857fbd325b2434c9418baf57602666f",
            "7d4407225204478981a5c1fc693903e2",
            "d76a94efd44540deaf1514e25c47d9db",
            "d5b6ac5416e44973b2dcc144807b9aaa",
            "4a000a74f93e4263bfbea21eb0c7b23a",
            "33e2d5de996a472890d32d7a65b9b82f",
            "99b59fd4f27b4d9388049ca14880455a"
          ]
        },
        "outputId": "5e76fb9d-de12-4881-db0e-9014655fd4d8"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "max_len = 512\n",
        "input_ids, attention_masks, text_len = tokenize_sentences(essaydf['essay'].tolist(), max_len)\n",
        "with torch.no_grad():\n",
        "  x_batch = input_ids.to(device)\n",
        "  attention_masks = attention_masks.to(device)\n",
        "  var = model.forward(x_batch, attention_masks)\n",
        "  print(\"predicted score\")\n",
        "  print(var.to('cpu'))\n",
        "  if(qnsNumber ==7):\n",
        "    var = model2.forward(x_batch, attention_masks)\n",
        "    print(\"predicted score multi model\")\n",
        "    result = var.to('cpu')\n",
        "    # print(result)\n",
        "    reformatted = np.zeros(test.shape[0])\n",
        "    reformatted[0] =result[0,4]\n",
        "    reformatted[1:] = result[0,:4]\n",
        "    print(reformatted)\n",
        "    print(\"real score\")\n",
        "    actualScore = getRealScore(essaydf)\n",
        "    print(actualScore)\n",
        "    #For multiple model the format is the following\n",
        "    # overall score, trait1, trait 2, trait 3, trait 4\n",
        "  else:\n",
        "    print(\"real score\")\n",
        "    print(np.array(essaydf['domain1_score'].values[0]))\n",
        "    \n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca8acc3062af4f76a11cf049c52e8448",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "760f786c3f3f44bc9252a2d74cc5833a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "473e4355e3ce431782ab098bc87c6e52",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "predicted score\n",
            "tensor([[4.5628]])\n",
            "real score\n",
            "5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JKqRsso2yKr"
      },
      "source": [
        "Predict any essay passages using bert below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSv6UjbdSfpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebe757a-c583-42fe-e804-c9c51c9250f3"
      },
      "source": [
        "# Trying out for personally sourced essays\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "max_len = 512\n",
        "input_ids, attention_masks, text_len = tokenize_sentences(essaydf['essay'].tolist(), max_len)\n",
        "#inputting your own essay in the dictionary below\n",
        "dictionaryEssay = {}\n",
        "dictionaryEssay['essay'] = [\"I am never paitent, but which I am it is only once and a while. Sitting down and not doing a thing drives me to be insane. When I’m doing nothing and it is really quiet. I get really bad headaches. In school for example and I’m waiting I get rigily like I tap my foot at tap my pencil and my desk. Tap ???everything things like that. I must do something or I go insane. Waiting in a doctors office, or flying on a plane for a while can get real boring. When I wait to long I feel lazy and tired because I’m not moving at all. I get buggy when I’m paitent, and waiting for something. It is usually no longer than @NUM1 min (half an hour) before I get real bad. I can’t be paitent very long or I get buggy. One way I help myself is playing sports and doing things constantive.\"]\n",
        "print(dictionaryEssay)\n",
        "dfown = pd.DataFrame.from_dict(dictionaryEssay)\n",
        "input_ids, attention_masks, text_len = tokenize_sentences(dfown['essay'].tolist(), max_len)\n",
        "with torch.no_grad():\n",
        "  x_batch = input_ids.to(device)\n",
        "  attention_masks = attention_masks.to(device)\n",
        "  var = model.forward(x_batch, attention_masks)\n",
        "  print(\"predicted score\")\n",
        "  print(var.to('cpu'))\n",
        "  #For multiple model the format is the following\n",
        "  # overall score, trait1, trait 2, trait 3, trait 4\n",
        "  if(qnsNumber ==7):\n",
        "    var = model2.forward(x_batch, attention_masks)\n",
        "    print(\"predicted score multi model\")\n",
        "    print(var.to('cpu'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'essay': ['I am never paitent, but which I am it is only once and a while. Sitting down and not doing a thing drives me to be insane. When I’m doing nothing and it is really quiet. I get really bad headaches. In school for example and I’m waiting I get rigily like I tap my foot at tap my pencil and my desk. Tap ???everything things like that. I must do something or I go insane. Waiting in a doctors office, or flying on a plane for a while can get real boring. When I wait to long I feel lazy and tired because I’m not moving at all. I get buggy when I’m paitent, and waiting for something. It is usually no longer than @NUM1 min (half an hour) before I get real bad. I can’t be paitent very long or I get buggy. One way I help myself is playing sports and doing things constantive.']}\n",
            "predicted score\n",
            "tensor([[2.3319]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GgZcGpiWWv0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}